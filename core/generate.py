import re
from core.chat import build_prompt
from core.model import llama_generate

def postprocess_response(text: str) -> str:
    text = text.strip()
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[.]{3,}", "...", text)
    text = text.split("–ò—Ç–∞–∫,")[0].strip()  # üí• –ñ—ë—Å—Ç–∫–æ —Ä–µ–∂–µ–º "–ò—Ç–∞–∫..."
    return text

def generate_response(chat: list, system_prompt: str, max_tokens: int = 600) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—è system_prompt + chat, —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –∫–∞–∫ –µ–¥–∏–Ω—ã–π —Ç–µ–∫—Å—Ç.
    """
    prompt_lines = [system_prompt.strip(), ""]
    round_number = 1

    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º —á–∞—Ç—ã –ø–æ–ø–∞—Ä–Ω–æ: user ‚Üí assistant
    for i in range(0, len(chat) - 1, 2):
        user_msg = chat[i]["content"].strip()
        assistant_msg = chat[i + 1]["content"].strip()
        prompt_lines.append(
            f"–†–∞—É–Ω–¥ {round_number}:\n"
            f"–ò–≥—Ä–æ–∫: {user_msg}\n"
            f"–û—Ç–≤–µ—Ç: {assistant_msg}"
        )
        round_number += 1

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–≥—Ä–æ–∫–∞ –±–µ–∑ –æ—Ç–≤–µ—Ç–∞
    if chat and chat[-1]["role"] == "user":
        user_msg = chat[-1]["content"].strip()
        prompt_lines.append(
            f"–†–∞—É–Ω–¥ {round_number}:\n"
            f"–ò–≥—Ä–æ–∫: {user_msg}\n"
            f"–û—Ç–≤–µ—Ç:"
        )

    full_prompt = "\n\n".join(prompt_lines)

    try:
        response = llama_generate(full_prompt, max_tokens=max_tokens)
        return response.strip()
    except Exception as e:
        return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"
