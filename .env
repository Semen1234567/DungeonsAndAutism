TELEGRAM_TOKEN=7762433709:AAHKrjazbDs5yhFHGe79xROWshwHU0EUNVQ
MODEL_PATH=D:\models\hugging-quants\Llama-3.2-1B-Instruct-Q8_0-GGUF\llama-3.2-1b-instruct-q8_0.gguf
N_CTX=1024          # 8 B «переварит» и 8-16 K, но 4 K быстрее
N_THREADS=8         # ≤ кол-ва физических ядер
N_BATCH=2048        # крупный батч = +15-30 % токенов/с
N_GPU_LAYERS=40      # 0 = чистый CPU; поставьте 40, если есть ~5 GB VRAM